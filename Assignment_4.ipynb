{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "YG3w7le4GVgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65e81700-8fe3-48a6-a81a-5592bad0bfef"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import sgd\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "from keras.callbacks import ModelCheckpoint,CSVLogger"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "TV88Eod0GfAs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mp_tVfxBGgCj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 12\n",
        "num_filter = 24\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QBfVuGTkGkWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "146eb65e-c100-4ad2-8698-03e4b7e10202"
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 20s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hQa1dSG_GpMz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Dense Block\n",
        "def add_denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l):\n",
        "        BatchNorm = BatchNormalization()(temp)\n",
        "        relu = Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "          Conv2D_3_3 = Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Zsy3zCLGtf-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def add_transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "      Conv2D_BottleNeck = Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    \n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3mdCMDefGvxr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = BatchNormalization()(input)\n",
        "    relu = Activation('relu')(BatchNorm)\n",
        "    AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = Flatten()(AvgPooling)\n",
        "    output = Dense(num_classes, activation='softmax')(flat)\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GxRRSWetGxzO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = add_denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = add_transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = add_denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = add_transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = add_denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = add_transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = add_denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNk_pKM9tjfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filepath=\"A4weights.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "trainlog = CSVLogger(\"training.log\")\n",
        "callbacks_list = [checkpoint,trainlog]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUZkf0lAG4La",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9860
        },
        "outputId": "521ee22f-320c-4e8a-c194-5fabb99be31f"
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 24)   648         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 12)   2592        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 32, 32, 12)   0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 36)   0           conv2d_1[0][0]                   \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 36)   144         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 36)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 12)   3888        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 32, 32, 12)   0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 48)   0           concatenate_1[0][0]              \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 48)   192         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 48)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5184        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 32, 32, 12)   0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 60)   0           concatenate_2[0][0]              \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 60)   240         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 60)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 12)   6480        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 32, 32, 12)   0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 72)   0           concatenate_3[0][0]              \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 72)   288         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 72)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 12)   7776        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 32, 32, 12)   0           conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 84)   0           concatenate_4[0][0]              \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 84)   336         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 84)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 12)   9072        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 32, 32, 12)   0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 96)   0           concatenate_5[0][0]              \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 96)   384         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 96)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 12)   10368       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 32, 32, 12)   0           conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 108)  0           concatenate_6[0][0]              \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 108)  432         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 108)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 12)   11664       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 32, 32, 12)   0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 120)  0           concatenate_7[0][0]              \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 120)  480         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 120)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 12)   12960       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 32, 32, 12)   0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 132)  0           concatenate_8[0][0]              \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 132)  528         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 132)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 12)   14256       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 32, 32, 12)   0           conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 144)  0           concatenate_9[0][0]              \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 144)  576         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 144)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 12)   15552       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 32, 32, 12)   0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 156)  0           concatenate_10[0][0]             \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 156)  624         concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 156)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 12)   16848       activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 32, 32, 12)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 32, 32, 168)  0           concatenate_11[0][0]             \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 168)  672         concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 168)  0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 12)   2016        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 32, 32, 12)   0           conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 16, 16, 12)   0           dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 12)   48          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 12)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 12)   1296        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 16, 16, 12)   0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 24)   0           average_pooling2d_1[0][0]        \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 24)   96          concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 24)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 12)   2592        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 16, 16, 12)   0           conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 36)   0           concatenate_13[0][0]             \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 36)   144         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 36)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 12)   3888        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 16, 16, 12)   0           conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 48)   0           concatenate_14[0][0]             \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 48)   192         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 48)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 12)   5184        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 16, 16, 12)   0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 60)   0           concatenate_15[0][0]             \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 60)   240         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 60)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 12)   6480        activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 16, 16, 12)   0           conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 72)   0           concatenate_16[0][0]             \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 72)   288         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 72)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 12)   7776        activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 16, 16, 12)   0           conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 84)   0           concatenate_17[0][0]             \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 84)   336         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 84)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 12)   9072        activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 16, 16, 12)   0           conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 96)   0           concatenate_18[0][0]             \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 96)   384         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 96)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 12)   10368       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 16, 16, 12)   0           conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 108)  0           concatenate_19[0][0]             \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 108)  432         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 108)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 12)   11664       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 16, 16, 12)   0           conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 120)  0           concatenate_20[0][0]             \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 120)  480         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 120)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 12)   12960       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 16, 16, 12)   0           conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 132)  0           concatenate_21[0][0]             \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 132)  528         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 132)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 12)   14256       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 16, 16, 12)   0           conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 144)  0           concatenate_22[0][0]             \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 144)  576         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 144)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 12)   15552       activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 16, 16, 12)   0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 16, 16, 156)  0           concatenate_23[0][0]             \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 156)  624         concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 156)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 12)   1872        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 16, 16, 12)   0           conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 8, 8, 12)     0           dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 12)     48          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 12)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 12)     1296        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 8, 8, 12)     0           conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 24)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 24)     96          concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 24)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 12)     2592        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 8, 8, 12)     0           conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 36)     0           concatenate_25[0][0]             \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 36)     144         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 36)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 12)     3888        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 8, 8, 12)     0           conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 48)     0           concatenate_26[0][0]             \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 48)     192         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 48)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 12)     5184        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_30 (Dropout)            (None, 8, 8, 12)     0           conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 60)     0           concatenate_27[0][0]             \n",
            "                                                                 dropout_30[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 60)     240         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 60)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 12)     6480        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_31 (Dropout)            (None, 8, 8, 12)     0           conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 72)     0           concatenate_28[0][0]             \n",
            "                                                                 dropout_31[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 72)     288         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 72)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 12)     7776        activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 8, 8, 12)     0           conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 84)     0           concatenate_29[0][0]             \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 84)     336         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 84)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 12)     9072        activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 8, 8, 12)     0           conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 96)     0           concatenate_30[0][0]             \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 96)     384         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 96)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 12)     10368       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 8, 8, 12)     0           conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 108)    0           concatenate_31[0][0]             \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 108)    432         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 108)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 12)     11664       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 8, 8, 12)     0           conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 120)    0           concatenate_32[0][0]             \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 120)    480         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 120)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 12)     12960       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 8, 8, 12)     0           conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 132)    0           concatenate_33[0][0]             \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 132)    528         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 132)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 12)     14256       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_37 (Dropout)            (None, 8, 8, 12)     0           conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 144)    0           concatenate_34[0][0]             \n",
            "                                                                 dropout_37[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 144)    576         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 144)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 12)     15552       activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 8, 8, 12)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 8, 8, 156)    0           concatenate_35[0][0]             \n",
            "                                                                 dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 8, 8, 156)    624         concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 8, 8, 156)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 8, 8, 12)     1872        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 8, 8, 12)     0           conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 4, 4, 12)     0           dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 12)     48          average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 12)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 12)     1296        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_40 (Dropout)            (None, 4, 4, 12)     0           conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 24)     0           average_pooling2d_3[0][0]        \n",
            "                                                                 dropout_40[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 24)     96          concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 24)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 12)     2592        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_41 (Dropout)            (None, 4, 4, 12)     0           conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 36)     0           concatenate_37[0][0]             \n",
            "                                                                 dropout_41[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 36)     144         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 36)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 12)     3888        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_42 (Dropout)            (None, 4, 4, 12)     0           conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 48)     0           concatenate_38[0][0]             \n",
            "                                                                 dropout_42[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 48)     192         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 48)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 12)     5184        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_43 (Dropout)            (None, 4, 4, 12)     0           conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 60)     0           concatenate_39[0][0]             \n",
            "                                                                 dropout_43[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 60)     240         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 60)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 12)     6480        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_44 (Dropout)            (None, 4, 4, 12)     0           conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 72)     0           concatenate_40[0][0]             \n",
            "                                                                 dropout_44[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 72)     288         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 72)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 12)     7776        activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_45 (Dropout)            (None, 4, 4, 12)     0           conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 84)     0           concatenate_41[0][0]             \n",
            "                                                                 dropout_45[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 84)     336         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 84)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 12)     9072        activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_46 (Dropout)            (None, 4, 4, 12)     0           conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 96)     0           concatenate_42[0][0]             \n",
            "                                                                 dropout_46[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 96)     384         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 96)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 12)     10368       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_47 (Dropout)            (None, 4, 4, 12)     0           conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 108)    0           concatenate_43[0][0]             \n",
            "                                                                 dropout_47[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 108)    432         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 108)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 12)     11664       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_48 (Dropout)            (None, 4, 4, 12)     0           conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 120)    0           concatenate_44[0][0]             \n",
            "                                                                 dropout_48[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 120)    480         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 120)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 12)     12960       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_49 (Dropout)            (None, 4, 4, 12)     0           conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 132)    0           concatenate_45[0][0]             \n",
            "                                                                 dropout_49[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 132)    528         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 132)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 12)     14256       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_50 (Dropout)            (None, 4, 4, 12)     0           conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 144)    0           concatenate_46[0][0]             \n",
            "                                                                 dropout_50[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 144)    576         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 144)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 4, 4, 12)     15552       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_51 (Dropout)            (None, 4, 4, 12)     0           conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_48 (Concatenate)    (None, 4, 4, 156)    0           concatenate_47[0][0]             \n",
            "                                                                 dropout_51[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 4, 4, 156)    624         concatenate_48[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 4, 4, 156)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 2, 2, 156)    0           activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 624)          0           average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           6250        flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 450,658\n",
            "Trainable params: 441,610\n",
            "Non-trainable params: 9,048\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CJk7-JavG6w4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=sgd(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2L5xIE3fG_q-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_gen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.25,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "validation_data_gen = ImageDataGenerator()\n",
        "train_generator = train_data_gen.flow(x_train, y_train, batch_size=batch_size)\n",
        "validation_generator = validation_data_gen.flow(x_test, y_test, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pm-rAKzLHBvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "246b7c1a-1524-41ba-bb83-83c9ffd527f8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model.fit_generator(train_generator, \n",
        "                    steps_per_epoch=train_generator.n//batch_size, \n",
        "                    epochs=epochs, \n",
        "                    callbacks=callbacks_list, \n",
        "                    verbose =1,\n",
        "                    validation_data=validation_generator, \n",
        "                    validation_steps=validation_generator.n//batch_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1562/1562 [==============================] - 315s 202ms/step - loss: 1.9072 - acc: 0.2792 - val_loss: 1.6687 - val_acc: 0.3779\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.37790, saving model to A4weights.best.hdf5\n",
            "Epoch 2/50\n",
            "1562/1562 [==============================] - 296s 190ms/step - loss: 1.6942 - acc: 0.3737 - val_loss: 1.9270 - val_acc: 0.3782\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.37790 to 0.37821, saving model to A4weights.best.hdf5\n",
            "Epoch 3/50\n",
            "1562/1562 [==============================] - 296s 189ms/step - loss: 1.5797 - acc: 0.4198 - val_loss: 2.2236 - val_acc: 0.3787\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.37821 to 0.37871, saving model to A4weights.best.hdf5\n",
            "Epoch 4/50\n",
            "1562/1562 [==============================] - 296s 190ms/step - loss: 1.4979 - acc: 0.4510 - val_loss: 2.1858 - val_acc: 0.3898\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.37871 to 0.38982, saving model to A4weights.best.hdf5\n",
            "Epoch 5/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 1.4338 - acc: 0.4769 - val_loss: 1.4348 - val_acc: 0.5243\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.38982 to 0.52434, saving model to A4weights.best.hdf5\n",
            "Epoch 6/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 1.3728 - acc: 0.4985 - val_loss: 1.9291 - val_acc: 0.4636\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.52434\n",
            "Epoch 7/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 1.3319 - acc: 0.5172 - val_loss: 1.4812 - val_acc: 0.5203\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.52434\n",
            "Epoch 8/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 1.2915 - acc: 0.5321 - val_loss: 1.7458 - val_acc: 0.5051\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.52434\n",
            "Epoch 9/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 1.2606 - acc: 0.5443 - val_loss: 1.7410 - val_acc: 0.5122\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.52434\n",
            "Epoch 10/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 1.2264 - acc: 0.5584 - val_loss: 2.1468 - val_acc: 0.4797\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.52434\n",
            "Epoch 11/50\n",
            "1562/1562 [==============================] - 294s 189ms/step - loss: 1.1973 - acc: 0.5685 - val_loss: 1.4994 - val_acc: 0.5686\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.52434 to 0.56861, saving model to A4weights.best.hdf5\n",
            "Epoch 12/50\n",
            "1562/1562 [==============================] - 297s 190ms/step - loss: 1.1755 - acc: 0.5792 - val_loss: 1.2689 - val_acc: 0.5985\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.56861 to 0.59846, saving model to A4weights.best.hdf5\n",
            "Epoch 13/50\n",
            "1562/1562 [==============================] - 297s 190ms/step - loss: 1.1566 - acc: 0.5838 - val_loss: 1.4150 - val_acc: 0.5888\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.59846\n",
            "Epoch 14/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 1.1363 - acc: 0.5947 - val_loss: 2.0369 - val_acc: 0.4986\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.59846\n",
            "Epoch 15/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 1.1208 - acc: 0.6004 - val_loss: 1.3942 - val_acc: 0.6006\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.59846 to 0.60056, saving model to A4weights.best.hdf5\n",
            "Epoch 16/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 1.1034 - acc: 0.6066 - val_loss: 1.5999 - val_acc: 0.5594\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.60056\n",
            "Epoch 17/50\n",
            "1562/1562 [==============================] - 290s 185ms/step - loss: 1.0817 - acc: 0.6139 - val_loss: 1.2800 - val_acc: 0.6203\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.60056 to 0.62029, saving model to A4weights.best.hdf5\n",
            "Epoch 18/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 1.0668 - acc: 0.6183 - val_loss: 1.5925 - val_acc: 0.5808\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.62029\n",
            "Epoch 19/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 1.0500 - acc: 0.6246 - val_loss: 1.3336 - val_acc: 0.6071\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.62029\n",
            "Epoch 20/50\n",
            "1562/1562 [==============================] - 293s 188ms/step - loss: 1.0406 - acc: 0.6284 - val_loss: 1.4471 - val_acc: 0.5946\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.62029\n",
            "Epoch 21/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 1.0207 - acc: 0.6361 - val_loss: 1.9277 - val_acc: 0.5350\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.62029\n",
            "Epoch 22/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 1.0099 - acc: 0.6383 - val_loss: 1.2831 - val_acc: 0.6403\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.62029 to 0.64032, saving model to A4weights.best.hdf5\n",
            "Epoch 23/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 0.9918 - acc: 0.6469 - val_loss: 1.3396 - val_acc: 0.6223\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.64032\n",
            "Epoch 24/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.9905 - acc: 0.6502 - val_loss: 1.2889 - val_acc: 0.6340\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.64032\n",
            "Epoch 25/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.9747 - acc: 0.6545 - val_loss: 1.1140 - val_acc: 0.6668\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.64032 to 0.66677, saving model to A4weights.best.hdf5\n",
            "Epoch 26/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.9631 - acc: 0.6564 - val_loss: 1.4140 - val_acc: 0.6215\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.66677\n",
            "Epoch 27/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.9460 - acc: 0.6648 - val_loss: 1.5576 - val_acc: 0.5904\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.66677\n",
            "Epoch 28/50\n",
            "1562/1562 [==============================] - 296s 189ms/step - loss: 0.9372 - acc: 0.6663 - val_loss: 1.1208 - val_acc: 0.6802\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.66677 to 0.68019, saving model to A4weights.best.hdf5\n",
            "Epoch 29/50\n",
            "1562/1562 [==============================] - 296s 190ms/step - loss: 0.9303 - acc: 0.6692 - val_loss: 0.9783 - val_acc: 0.6978\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.68019 to 0.69782, saving model to A4weights.best.hdf5\n",
            "Epoch 30/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.9260 - acc: 0.6732 - val_loss: 1.7228 - val_acc: 0.5847\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.69782\n",
            "Epoch 31/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.9131 - acc: 0.6784 - val_loss: 1.3052 - val_acc: 0.6561\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.69782\n",
            "Epoch 32/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.8970 - acc: 0.6815 - val_loss: 1.1469 - val_acc: 0.6682\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.69782\n",
            "Epoch 33/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.8895 - acc: 0.6836 - val_loss: 1.1674 - val_acc: 0.6730\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.69782\n",
            "Epoch 34/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.8791 - acc: 0.6882 - val_loss: 1.0441 - val_acc: 0.6980\n",
            "\n",
            "Epoch 00034: val_acc improved from 0.69782 to 0.69802, saving model to A4weights.best.hdf5\n",
            "Epoch 35/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.8753 - acc: 0.6903 - val_loss: 0.8484 - val_acc: 0.7337\n",
            "\n",
            "Epoch 00035: val_acc improved from 0.69802 to 0.73367, saving model to A4weights.best.hdf5\n",
            "Epoch 36/50\n",
            "1562/1562 [==============================] - 293s 187ms/step - loss: 0.8639 - acc: 0.6928 - val_loss: 1.3749 - val_acc: 0.6454\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.73367\n",
            "Epoch 37/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.8590 - acc: 0.6974 - val_loss: 1.4026 - val_acc: 0.6444\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.73367\n",
            "Epoch 38/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.8515 - acc: 0.6996 - val_loss: 1.2821 - val_acc: 0.6596\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.73367\n",
            "Epoch 39/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.8432 - acc: 0.7016 - val_loss: 0.9764 - val_acc: 0.7124\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.73367\n",
            "Epoch 40/50\n",
            "1562/1562 [==============================] - 291s 187ms/step - loss: 0.8293 - acc: 0.7064 - val_loss: 1.1682 - val_acc: 0.6785\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.73367\n",
            "Epoch 41/50\n",
            "1562/1562 [==============================] - 293s 187ms/step - loss: 0.8311 - acc: 0.7056 - val_loss: 1.1948 - val_acc: 0.6795\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.73367\n",
            "Epoch 42/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.8276 - acc: 0.7096 - val_loss: 1.2120 - val_acc: 0.6784\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.73367\n",
            "Epoch 43/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.8144 - acc: 0.7128 - val_loss: 0.9317 - val_acc: 0.7281\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.73367\n",
            "Epoch 44/50\n",
            "1562/1562 [==============================] - 293s 187ms/step - loss: 0.8090 - acc: 0.7157 - val_loss: 1.2546 - val_acc: 0.6764\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.73367\n",
            "Epoch 45/50\n",
            "1562/1562 [==============================] - 293s 188ms/step - loss: 0.8069 - acc: 0.7158 - val_loss: 1.1740 - val_acc: 0.6867\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.73367\n",
            "Epoch 46/50\n",
            "1562/1562 [==============================] - 293s 187ms/step - loss: 0.8011 - acc: 0.7189 - val_loss: 1.0844 - val_acc: 0.7093\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.73367\n",
            "Epoch 47/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.7913 - acc: 0.7202 - val_loss: 0.9837 - val_acc: 0.7193\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.73367\n",
            "Epoch 48/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.7879 - acc: 0.7230 - val_loss: 0.9441 - val_acc: 0.7293\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.73367\n",
            "Epoch 49/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.7830 - acc: 0.7246 - val_loss: 0.8991 - val_acc: 0.7432\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.73367 to 0.74319, saving model to A4weights.best.hdf5\n",
            "Epoch 50/50\n",
            "1562/1562 [==============================] - 297s 190ms/step - loss: 0.7781 - acc: 0.7271 - val_loss: 1.0337 - val_acc: 0.7220\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.74319\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3caebab6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "vzsrgyJ1utw0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0TuZQxcdHH1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"A4weights.best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rh8E6dHum7j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3434
        },
        "outputId": "113383e8-bd01-40b8-9d55-ee8d7de54fa7"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator, \n",
        "                    steps_per_epoch=train_generator.n//batch_size, \n",
        "                    epochs=epochs, \n",
        "                    callbacks=callbacks_list, \n",
        "                    verbose =1,\n",
        "                    validation_data=validation_generator, \n",
        "                    validation_steps=validation_generator.n//batch_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1562/1562 [==============================] - 315s 201ms/step - loss: 0.7816 - acc: 0.7233 - val_loss: 1.3173 - val_acc: 0.6750\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.74319\n",
            "Epoch 2/50\n",
            "1562/1562 [==============================] - 298s 191ms/step - loss: 0.7685 - acc: 0.7296 - val_loss: 1.1134 - val_acc: 0.6896\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.74319\n",
            "Epoch 3/50\n",
            "1562/1562 [==============================] - 298s 191ms/step - loss: 0.7697 - acc: 0.7304 - val_loss: 1.0291 - val_acc: 0.7241\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.74319\n",
            "Epoch 4/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.7538 - acc: 0.7349 - val_loss: 1.1151 - val_acc: 0.7020\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.74319\n",
            "Epoch 5/50\n",
            "1562/1562 [==============================] - 290s 185ms/step - loss: 0.7581 - acc: 0.7370 - val_loss: 0.8593 - val_acc: 0.7540\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.74319 to 0.75401, saving model to A4weights.best.hdf5\n",
            "Epoch 6/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.7546 - acc: 0.7350 - val_loss: 1.1488 - val_acc: 0.6946\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.75401\n",
            "Epoch 7/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.7521 - acc: 0.7360 - val_loss: 0.9535 - val_acc: 0.7352\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.75401\n",
            "Epoch 8/50\n",
            "1562/1562 [==============================] - 293s 188ms/step - loss: 0.7436 - acc: 0.7390 - val_loss: 0.9430 - val_acc: 0.7364\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.75401\n",
            "Epoch 9/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.7418 - acc: 0.7413 - val_loss: 0.9015 - val_acc: 0.7477\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.75401\n",
            "Epoch 10/50\n",
            "1562/1562 [==============================] - 299s 192ms/step - loss: 0.7309 - acc: 0.7434 - val_loss: 1.0926 - val_acc: 0.7142\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.75401\n",
            "Epoch 11/50\n",
            "1562/1562 [==============================] - 299s 191ms/step - loss: 0.7283 - acc: 0.7461 - val_loss: 0.9796 - val_acc: 0.7344\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.75401\n",
            "Epoch 12/50\n",
            "1562/1562 [==============================] - 298s 191ms/step - loss: 0.7242 - acc: 0.7466 - val_loss: 0.9107 - val_acc: 0.7364\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.75401\n",
            "Epoch 13/50\n",
            "1562/1562 [==============================] - 298s 191ms/step - loss: 0.7206 - acc: 0.7464 - val_loss: 0.9847 - val_acc: 0.7378\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.75401\n",
            "Epoch 14/50\n",
            "1562/1562 [==============================] - 296s 190ms/step - loss: 0.7148 - acc: 0.7501 - val_loss: 1.2994 - val_acc: 0.6931\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.75401\n",
            "Epoch 15/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.7122 - acc: 0.7489 - val_loss: 1.0921 - val_acc: 0.7156\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.75401\n",
            "Epoch 16/50\n",
            "1562/1562 [==============================] - 293s 188ms/step - loss: 0.7097 - acc: 0.7513 - val_loss: 0.8260 - val_acc: 0.7621\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.75401 to 0.76212, saving model to A4weights.best.hdf5\n",
            "Epoch 17/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.7124 - acc: 0.7518 - val_loss: 0.7738 - val_acc: 0.7735\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.76212 to 0.77354, saving model to A4weights.best.hdf5\n",
            "Epoch 18/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.6959 - acc: 0.7559 - val_loss: 1.0048 - val_acc: 0.7308\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.77354\n",
            "Epoch 19/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6952 - acc: 0.7572 - val_loss: 0.9507 - val_acc: 0.7485\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.77354\n",
            "Epoch 20/50\n",
            "1562/1562 [==============================] - 290s 185ms/step - loss: 0.6940 - acc: 0.7571 - val_loss: 0.8016 - val_acc: 0.7659\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.77354\n",
            "Epoch 21/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 0.6923 - acc: 0.7577 - val_loss: 0.7329 - val_acc: 0.7820\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.77354 to 0.78195, saving model to A4weights.best.hdf5\n",
            "Epoch 22/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6918 - acc: 0.7575 - val_loss: 0.8098 - val_acc: 0.7691\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.78195\n",
            "Epoch 23/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6881 - acc: 0.7603 - val_loss: 0.7497 - val_acc: 0.7812\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.78195\n",
            "Epoch 24/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6823 - acc: 0.7621 - val_loss: 0.7194 - val_acc: 0.7898\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.78195 to 0.78976, saving model to A4weights.best.hdf5\n",
            "Epoch 25/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.6790 - acc: 0.7632 - val_loss: 0.6723 - val_acc: 0.8024\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.78976 to 0.80238, saving model to A4weights.best.hdf5\n",
            "Epoch 26/50\n",
            "1562/1562 [==============================] - 290s 185ms/step - loss: 0.6776 - acc: 0.7641 - val_loss: 0.8629 - val_acc: 0.7673\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.80238\n",
            "Epoch 27/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6709 - acc: 0.7653 - val_loss: 0.9666 - val_acc: 0.7465\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.80238\n",
            "Epoch 28/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6727 - acc: 0.7648 - val_loss: 0.8375 - val_acc: 0.7688\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.80238\n",
            "Epoch 29/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6650 - acc: 0.7688 - val_loss: 0.7589 - val_acc: 0.7796\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.80238\n",
            "Epoch 30/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6638 - acc: 0.7705 - val_loss: 0.9264 - val_acc: 0.7485\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.80238\n",
            "Epoch 31/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6639 - acc: 0.7665 - val_loss: 0.8072 - val_acc: 0.7764\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.80238\n",
            "Epoch 32/50\n",
            "1562/1562 [==============================] - 289s 185ms/step - loss: 0.6627 - acc: 0.7686 - val_loss: 0.9434 - val_acc: 0.7505\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.80238\n",
            "Epoch 33/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 0.6549 - acc: 0.7709 - val_loss: 0.9961 - val_acc: 0.7397\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.80238\n",
            "Epoch 34/50\n",
            "1562/1562 [==============================] - 293s 187ms/step - loss: 0.6541 - acc: 0.7707 - val_loss: 0.8082 - val_acc: 0.7744\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.80238\n",
            "Epoch 35/50\n",
            "1562/1562 [==============================] - 293s 187ms/step - loss: 0.6441 - acc: 0.7751 - val_loss: 0.7608 - val_acc: 0.7834\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.80238\n",
            "Epoch 36/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6461 - acc: 0.7750 - val_loss: 1.1315 - val_acc: 0.7240\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.80238\n",
            "Epoch 37/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.6463 - acc: 0.7759 - val_loss: 0.7246 - val_acc: 0.7925\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.80238\n",
            "Epoch 38/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 0.6414 - acc: 0.7742 - val_loss: 0.8439 - val_acc: 0.7751\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.80238\n",
            "Epoch 39/50\n",
            "1562/1562 [==============================] - 290s 186ms/step - loss: 0.6374 - acc: 0.7761 - val_loss: 0.7274 - val_acc: 0.7943\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.80238\n",
            "Epoch 40/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.6346 - acc: 0.7777 - val_loss: 0.9361 - val_acc: 0.7537\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.80238\n",
            "Epoch 41/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6318 - acc: 0.7780 - val_loss: 0.9278 - val_acc: 0.7560\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.80238\n",
            "Epoch 42/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6341 - acc: 0.7788 - val_loss: 0.9243 - val_acc: 0.7623\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.80238\n",
            "Epoch 43/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6298 - acc: 0.7806 - val_loss: 0.9241 - val_acc: 0.7580\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.80238\n",
            "Epoch 44/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6231 - acc: 0.7818 - val_loss: 0.8798 - val_acc: 0.7651\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.80238\n",
            "Epoch 45/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6274 - acc: 0.7811 - val_loss: 0.9268 - val_acc: 0.7599\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.80238\n",
            "Epoch 46/50\n",
            "1562/1562 [==============================] - 291s 187ms/step - loss: 0.6202 - acc: 0.7824 - val_loss: 0.6968 - val_acc: 0.8022\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.80238\n",
            "Epoch 47/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.6245 - acc: 0.7824 - val_loss: 0.8294 - val_acc: 0.7698\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.80238\n",
            "Epoch 48/50\n",
            "1562/1562 [==============================] - 291s 187ms/step - loss: 0.6214 - acc: 0.7828 - val_loss: 0.9295 - val_acc: 0.7605\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.80238\n",
            "Epoch 49/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6180 - acc: 0.7848 - val_loss: 0.8701 - val_acc: 0.7689\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.80238\n",
            "Epoch 50/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6174 - acc: 0.7869 - val_loss: 0.7857 - val_acc: 0.7792\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.80238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3ca1a633c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "r8lZQHpwexjA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"A4weights.best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wq0oTv9GexFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3179
        },
        "outputId": "cf897a7d-196d-4dc2-9641-0e72a77f7570"
      },
      "cell_type": "code",
      "source": [
        "model.fit_generator(train_generator, \n",
        "                    steps_per_epoch=train_generator.n//batch_size, \n",
        "                    epochs=epochs, \n",
        "                    callbacks=callbacks_list, \n",
        "                    verbose =1,\n",
        "                    validation_data=validation_generator, \n",
        "                    validation_steps=validation_generator.n//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1562/1562 [==============================] - 304s 195ms/step - loss: 0.6796 - acc: 0.7627 - val_loss: 1.0636 - val_acc: 0.7290\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.80238\n",
            "Epoch 2/50\n",
            "1562/1562 [==============================] - 291s 186ms/step - loss: 0.6704 - acc: 0.7657 - val_loss: 0.8962 - val_acc: 0.7604\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.80238\n",
            "Epoch 3/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6733 - acc: 0.7645 - val_loss: 0.8706 - val_acc: 0.7603\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.80238\n",
            "Epoch 4/50\n",
            "1562/1562 [==============================] - 291s 187ms/step - loss: 0.6675 - acc: 0.7682 - val_loss: 0.9744 - val_acc: 0.7348\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.80238\n",
            "Epoch 5/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6648 - acc: 0.7677 - val_loss: 0.6552 - val_acc: 0.8048\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.80238 to 0.80479, saving model to A4weights.best.hdf5\n",
            "Epoch 6/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6579 - acc: 0.7700 - val_loss: 0.8176 - val_acc: 0.7710\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.80479\n",
            "Epoch 7/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6507 - acc: 0.7733 - val_loss: 0.9621 - val_acc: 0.7448\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.80479\n",
            "Epoch 8/50\n",
            "1562/1562 [==============================] - 293s 188ms/step - loss: 0.6604 - acc: 0.7680 - val_loss: 0.7367 - val_acc: 0.7818\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.80479\n",
            "Epoch 9/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.6526 - acc: 0.7717 - val_loss: 0.9348 - val_acc: 0.7464\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.80479\n",
            "Epoch 10/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.6547 - acc: 0.7735 - val_loss: 0.8589 - val_acc: 0.7607\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.80479\n",
            "Epoch 11/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.6515 - acc: 0.7737 - val_loss: 0.7497 - val_acc: 0.7764\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.80479\n",
            "Epoch 12/50\n",
            "1562/1562 [==============================] - 285s 182ms/step - loss: 0.6394 - acc: 0.7782 - val_loss: 0.8380 - val_acc: 0.7682\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.80479\n",
            "Epoch 13/50\n",
            "1562/1562 [==============================] - 286s 183ms/step - loss: 0.6441 - acc: 0.7755 - val_loss: 0.8392 - val_acc: 0.7702\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.80479\n",
            "Epoch 14/50\n",
            "1562/1562 [==============================] - 286s 183ms/step - loss: 0.6372 - acc: 0.7762 - val_loss: 0.8850 - val_acc: 0.7585\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.80479\n",
            "Epoch 15/50\n",
            "1562/1562 [==============================] - 288s 184ms/step - loss: 0.6410 - acc: 0.7754 - val_loss: 1.0474 - val_acc: 0.7293\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.80479\n",
            "Epoch 16/50\n",
            "1562/1562 [==============================] - 287s 184ms/step - loss: 0.6338 - acc: 0.7799 - val_loss: 0.8069 - val_acc: 0.7781\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.80479\n",
            "Epoch 17/50\n",
            "1562/1562 [==============================] - 284s 182ms/step - loss: 0.6321 - acc: 0.7781 - val_loss: 0.9096 - val_acc: 0.7605\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.80479\n",
            "Epoch 18/50\n",
            "1562/1562 [==============================] - 284s 182ms/step - loss: 0.6333 - acc: 0.7801 - val_loss: 0.6963 - val_acc: 0.7963\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.80479\n",
            "Epoch 19/50\n",
            "1562/1562 [==============================] - 284s 182ms/step - loss: 0.6222 - acc: 0.7824 - val_loss: 0.7967 - val_acc: 0.7756\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.80479\n",
            "Epoch 20/50\n",
            "1562/1562 [==============================] - 284s 182ms/step - loss: 0.6227 - acc: 0.7837 - val_loss: 0.8171 - val_acc: 0.7731\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.80479\n",
            "Epoch 21/50\n",
            "1562/1562 [==============================] - 284s 182ms/step - loss: 0.6271 - acc: 0.7792 - val_loss: 0.7923 - val_acc: 0.7854\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.80479\n",
            "Epoch 22/50\n",
            "1562/1562 [==============================] - 274s 175ms/step - loss: 0.6191 - acc: 0.7833 - val_loss: 0.7215 - val_acc: 0.7950\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.80479\n",
            "Epoch 23/50\n",
            "1562/1562 [==============================] - 279s 179ms/step - loss: 0.6194 - acc: 0.7843 - val_loss: 0.6614 - val_acc: 0.8074\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.80479 to 0.80739, saving model to A4weights.best.hdf5\n",
            "Epoch 24/50\n",
            "1562/1562 [==============================] - 286s 183ms/step - loss: 0.6157 - acc: 0.7842 - val_loss: 0.7187 - val_acc: 0.7980\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.80739\n",
            "Epoch 25/50\n",
            "1562/1562 [==============================] - 285s 182ms/step - loss: 0.6076 - acc: 0.7883 - val_loss: 0.9011 - val_acc: 0.7613\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.80739\n",
            "Epoch 26/50\n",
            "1562/1562 [==============================] - 285s 183ms/step - loss: 0.6096 - acc: 0.7881 - val_loss: 0.9037 - val_acc: 0.7625\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.80739\n",
            "Epoch 27/50\n",
            "1562/1562 [==============================] - 283s 181ms/step - loss: 0.6108 - acc: 0.7867 - val_loss: 0.8513 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.80739\n",
            "Epoch 28/50\n",
            "1562/1562 [==============================] - 285s 182ms/step - loss: 0.6078 - acc: 0.7868 - val_loss: 0.7630 - val_acc: 0.7943\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.80739\n",
            "Epoch 29/50\n",
            "1562/1562 [==============================] - 287s 184ms/step - loss: 0.6087 - acc: 0.7885 - val_loss: 0.7979 - val_acc: 0.7810\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.80739\n",
            "Epoch 30/50\n",
            "1562/1562 [==============================] - 286s 183ms/step - loss: 0.6044 - acc: 0.7893 - val_loss: 0.6971 - val_acc: 0.8020\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.80739\n",
            "Epoch 31/50\n",
            "1562/1562 [==============================] - 285s 182ms/step - loss: 0.6095 - acc: 0.7887 - val_loss: 0.8569 - val_acc: 0.7692\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.80739\n",
            "Epoch 32/50\n",
            "1562/1562 [==============================] - 285s 183ms/step - loss: 0.5999 - acc: 0.7939 - val_loss: 0.5921 - val_acc: 0.8244\n",
            "\n",
            "Epoch 00032: val_acc improved from 0.80739 to 0.82442, saving model to A4weights.best.hdf5\n",
            "Epoch 33/50\n",
            "1562/1562 [==============================] - 281s 180ms/step - loss: 0.5951 - acc: 0.7921 - val_loss: 0.9628 - val_acc: 0.7581\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.82442\n",
            "Epoch 34/50\n",
            "1562/1562 [==============================] - 284s 182ms/step - loss: 0.5967 - acc: 0.7932 - val_loss: 0.6255 - val_acc: 0.8202\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.82442\n",
            "Epoch 35/50\n",
            "1562/1562 [==============================] - 285s 182ms/step - loss: 0.5930 - acc: 0.7935 - val_loss: 0.7146 - val_acc: 0.8001\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.82442\n",
            "Epoch 36/50\n",
            "1562/1562 [==============================] - 284s 182ms/step - loss: 0.5908 - acc: 0.7943 - val_loss: 0.9171 - val_acc: 0.7576\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.82442\n",
            "Epoch 37/50\n",
            "1562/1562 [==============================] - 286s 183ms/step - loss: 0.5913 - acc: 0.7959 - val_loss: 0.8192 - val_acc: 0.7807\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.82442\n",
            "Epoch 38/50\n",
            "1562/1562 [==============================] - 293s 188ms/step - loss: 0.5876 - acc: 0.7955 - val_loss: 0.7635 - val_acc: 0.7920\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.82442\n",
            "Epoch 39/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.5893 - acc: 0.7937 - val_loss: 0.7556 - val_acc: 0.7901\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.82442\n",
            "Epoch 40/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.5881 - acc: 0.7946 - val_loss: 0.6723 - val_acc: 0.8130\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.82442\n",
            "Epoch 41/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.5893 - acc: 0.7946 - val_loss: 0.6559 - val_acc: 0.8065\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.82442\n",
            "Epoch 42/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.5873 - acc: 0.7971 - val_loss: 0.8329 - val_acc: 0.7701\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.82442\n",
            "Epoch 43/50\n",
            "1562/1562 [==============================] - 294s 188ms/step - loss: 0.5811 - acc: 0.7990 - val_loss: 0.6616 - val_acc: 0.8081\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.82442\n",
            "Epoch 44/50\n",
            "1562/1562 [==============================] - 295s 189ms/step - loss: 0.5790 - acc: 0.7982 - val_loss: 0.7984 - val_acc: 0.7877\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.82442\n",
            "Epoch 45/50\n",
            "1562/1562 [==============================] - 292s 187ms/step - loss: 0.5749 - acc: 0.7985 - val_loss: 0.6889 - val_acc: 0.8092\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.82442\n",
            "Epoch 46/50\n",
            "1562/1562 [==============================] - 288s 184ms/step - loss: 0.5697 - acc: 0.8021 - val_loss: 0.8794 - val_acc: 0.7656\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.82442\n",
            "Epoch 47/50\n",
            " 991/1562 [==================>...........] - ETA: 1:39 - loss: 0.5739 - acc: 0.7991"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}